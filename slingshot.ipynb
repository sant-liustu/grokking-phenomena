{"cells":[{"id":"55ebcb8c-8116-498c-b689-22a75e4d5996","cell_type":"markdown","source":"","metadata":{}},{"id":"0151a2e4-9e23-4703-9bd3-5e69ec96021c","cell_type":"markdown","source":"p97 wd0.0 dp0.0 bs512 va没有起色","metadata":{}},{"id":"3218d056-74cc-4eb5-a35b-a2db9cd0cb20","cell_type":"code","source":"","metadata":{},"execution_count":null},{"id":"933117fc-3bc2-4004-865f-ba5b1a64c352","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport os\nimport time\nprint(torch.cuda.is_available())\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n# 1. Data Generation\nclass ModularAdditionDataset(Dataset):\n    def __init__(self, p, embed_dim):\n        assert embed_dim > p + 2, \"Embedding dimension must be greater than p + 2\"\n        self.p = p\n        self.embed_dim = embed_dim\n        \n        x = torch.arange(p)\n        y = torch.arange(p)\n        self.x, self.y = torch.meshgrid(x, y, indexing='ij')\n        self.result = (self.x + self.y) % p\n        \n        # Flatten tensors\n        self.x = self.x.reshape(-1)\n        self.y = self.y.reshape(-1)\n        self.result = self.result.reshape(-1)\n        \n        # Convert to one-hot vectors\n        self.x = nn.functional.one_hot(self.x, self.embed_dim)\n        self.y = nn.functional.one_hot(self.y, self.embed_dim)\n        \n        # Create one-hot vectors for '+' and '='\n        self.plus = nn.functional.one_hot(torch.tensor([p]), embed_dim)\n        self.equals = nn.functional.one_hot(torch.tensor([p + 1]), embed_dim)\n        \n    def __len__(self):\n        return len(self.result)\n        \n    def __getitem__(self, idx):\n        x_onehot = self.x[idx].float()\n        y_onehot = self.y[idx].float()\n        result = self.result[idx]\n        # Concatenate x, +, y, =, result\n        input_vector = torch.stack([x_onehot, self.plus.squeeze(0).float(), y_onehot, self.equals.squeeze(0).float()])\n        \n        return input_vector, result \n\n# 2. Transformer Model\nclass DecoderBlock(nn.Module):\n    def __init__(self, dim_model: int, n_heads: int):\n        super().__init__()\n\n        self.self_attn = nn.MultiheadAttention(dim_model, n_heads)\n        self.self_attn_norm = nn.LayerNorm(dim_model)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim_model, dim_model * 4),\n            #nn.Dropout(0.0),  # 添加dropout\n            nn.GELU(),\n            nn.Linear(dim_model * 4, dim_model),\n            nn.Dropout(0.0)  # 添加dropout\n        )\n        self.ffn_norm = nn.LayerNorm(dim_model)\n\n    def forward(self, x):\n        attn_mask = torch.full(\n            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n        )\n        attn_mask = torch.triu(attn_mask, diagonal=1)\n        \n        a1, _ = self.self_attn(x, x, x, attn_mask=attn_mask)\n        a1 = self.self_attn_norm(x + a1)\n        a2 = self.ffn(a1)\n        a2 = self.ffn_norm(a1 + a2)\n\n        return a2\n\nclass SimpleTransformer(nn.Module):\n    def __init__(self, num_layers: int, dim_model: int, num_heads: int, num_tokens: int):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            *[DecoderBlock(dim_model, num_heads) for _ in range(num_layers)],\n            nn.LayerNorm(dim_model),\n            nn.Linear(dim_model, num_tokens)\n        )\n\n    def _position_encoding(self, seq_len, dim_model):\n        pos = np.arange(seq_len)[:, np.newaxis]\n        i = np.arange(dim_model)[np.newaxis, :]\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / dim_model)\n        angle_rads = pos * angle_rates\n\n        # apply sin to even indices in the array; 2i\n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n        # apply cos to odd indices in the array; 2i+1\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n        pos_encoding = angle_rads[np.newaxis, ...]\n        return torch.tensor(pos_encoding, dtype=torch.float32)\n\n    def forward(self, x):\n        #x是事先已经embedding好的，只需再加上position embedding\n        position_embedding = self._position_encoding(x.shape[1], x.shape[2]).to(x.device)\n\n        embedding = x + position_embedding\n\n        embedding = embedding.permute(1, 0, 2)  # (seq_len, batch_size, dim_model)\n\n        output = self.model(embedding)\n        output = output[-1, :, :]  # (batch_size, num_tokens)\n\n\n        return output\n    \n\n# 3. Training Loop\ndef train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, num_epochs, device, save_path):\n    train_accs = []\n    val_accs = []\n    feature_changes = [0] #添加特征变化变量\n    layer_norms = []  #添加层权重范数\n    steps = []\n    current_step = 0\n    feature_change = 0.0\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n\n        model.train()\n        for x, labels in train_loader:\n            x = x.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            if current_step % 100 == 0:\n                with torch.no_grad():\n                    train_pred = outputs.argmax(dim=1)\n                    train_acc = (train_pred == labels).float().mean().item()\n\n                    model.eval()\n                    val_correct = 0\n                    val_total = 0\n                    for val_x, val_labels in val_loader:\n                        val_x = val_x.to(device)\n                        val_labels = val_labels.to(device)\n                        val_outputs = model(val_x)\n                        val_pred = val_outputs.argmax(dim=1)\n                        val_correct += (val_pred == val_labels).sum().item()\n                        val_total += val_labels.size(0)\n                    val_acc = val_correct / val_total\n                    model.train()\n\n                    train_accs.append(train_acc)\n                    val_accs.append(val_acc)\n\n                    # 计算特征变化\n                    if current_step > 0:\n                        feature_change = torch.norm(model.model[-1].weight - prev_weights) / torch.norm(prev_weights)\n                        feature_changes.append(feature_change.item())\n                    prev_weights = model.model[-1].weight.clone()\n\n                    # 计算层权重范数\n                    layer_norm = torch.norm(model.model[-1].weight)\n                    layer_norms.append(layer_norm.item())\n\n                    steps.append(current_step)\n                    end_time = time.time()\n                    step100_duration = end_time - start_time\n                    start_time = end_time\n\n                    print(f'Step {current_step}, Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}, Feature Change: {feature_change:.3f}, Layer Norm: {layer_norm:.3f}, Time per 100 steps: {step100_duration:.2f}s')\n\n            current_step += 1\n\n        if (epoch + 1) % 10 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_accs': train_accs,\n                'val_accs': val_accs,\n                'feature_changes': feature_changes,\n                'layer_norms': layer_norms,\n                'steps': steps\n            }, os.path.join(save_path, f'checkpoint_epoch_{epoch+1}.pt'))\n\n    return steps, train_accs, val_accs, feature_changes, layer_norms\n\n# 修改 run_experiment 函数\ndef run_experiment(p=53, hidden_dim=128, num_heads=4, num_layers=2,\n                  batch_size=512, lr=8e-4, weight_decay=0.0, training_fraction=0.3,\n                  num_epochs=10000, device='cpu', save_path='checkpoints'):\n    os.makedirs(save_path, exist_ok=True)\n\n    dataset = ModularAdditionDataset(p, hidden_dim)\n    train_size = int(training_fraction * len(dataset))\n    val_size = len(dataset) - train_size\n    random_seed = 42\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(random_seed))\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    model = SimpleTransformer(num_layers=num_layers, dim_model=hidden_dim, num_heads=num_heads, num_tokens=p+2).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    steps, train_accs, val_accs, feature_changes, layer_norms = train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, num_epochs, device, save_path)\n\n \n","metadata":{},"outputs":[{"id":"fc61b6cc-6245325c2247870fa32d48ef_79_118","output_type":"stream","name":"stdout","text":"False\n","data":{"name":"stdout","text":"False\n"},"meta":{},"parent_header":{"msg_id":"fc61b6cc-6245325c2247870fa32d48ef_79_118","msg_type":"stream","username":"username","session":"fc61b6cc-6245325c2247870fa32d48ef","date":"2024-12-31T15:18:32.049919Z","version":"5.3"}},{"id":"fc61b6cc-6245325c2247870fa32d48ef_79_119","output_type":"execute_reply","data":{"status":"ok","execution_count":9,"user_expressions":{},"payload":[]},"meta":{"started":"2024-12-31T15:18:32.029001Z","dependencies_met":true,"engine":"e239cf83-dc0e-40d5-ab36-f0cfe27b3b11","status":"ok"},"parent_header":{"msg_id":"fc61b6cc-6245325c2247870fa32d48ef_79_119","msg_type":"execute_reply","username":"username","session":"fc61b6cc-6245325c2247870fa32d48ef","date":"2024-12-31T15:18:32.051114Z","version":"5.3"}}],"execution_count":9},{"id":"a2a740bf-0404-46d4-8813-8eeffa041c46","cell_type":"code","source":"\nif __name__ == \"__main__\":\n    run_experiment()","metadata":{},"outputs":[{"id":"fc61b6cc-6245325c2247870fa32d48ef_79_123","output_type":"stream","name":"stdout","text":"Step 0, Train Acc: 0.020, Val Acc: 0.016, Feature Change: 0.000, Layer Norm: 4.302, Time per 100 steps: 0.28s\n","data":{"name":"stdout","text":"Step 0, Train Acc: 0.020, Val Acc: 0.016, Feature Change: 0.000, Layer Norm: 4.302, Time per 100 steps: 0.28s\n"},"meta":{},"parent_header":{"msg_id":"fc61b6cc-6245325c2247870fa32d48ef_79_123","msg_type":"stream","username":"username","session":"fc61b6cc-6245325c2247870fa32d48ef","date":"2024-12-31T15:18:32.541717Z","version":"5.3"}}],"execution_count":10},{"id":"52c92339-b40d-48e8-b1d6-3489c692d302","cell_type":"code","source":"","metadata":{},"execution_count":null}],"metadata":{},"nbformat":4,"nbformat_minor":5}